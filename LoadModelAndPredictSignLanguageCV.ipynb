{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/philipp-ding/AI/blob/main/LoadModelAndPredictSignLanguageCV.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Projectrealisierung:\n",
        "\n",
        "**Aufgabe:**\n",
        "Erstellung einer Applikation mit Kamera Input, die Zeichensprache von nicht-Hörenden / nicht-Sprechenden Personen konvertiert und diese anderen Meeting-Teilnehmern als Untertitel ausgibt. Bereitstellung eines Frontends zu Demonstrationszwecken und einer API, um die Modelle in andere Applikationen wie Teams oder Discord zu integrieren."
      ],
      "metadata": {
        "id": "dzd-PoCH_MX2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZF2TD1BJuPSr"
      },
      "source": [
        "how to execute this notebook:\n",
        "1. Go to https://drive.google.com/file/d/1be8Cai-xqnSQKJQmnVxAuEFmm_xU1N9Z/view?usp=sharing (containing the zip file with the 100 glosses), then add a shortcut to this file in your drive so that it is integrated into your google drive\n",
        "2. Go to https://drive.google.com/file/d/1EIE3FUYi_hvIxAEqxaxxmrHlE1GDmcND/view?usp=sharing (containing the WASL100.json file), then add a shortcut to this file in your drive\n",
        "3. Simply run the cells in this notebook\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6-S7oMWHE1K"
      },
      "source": [
        "# 1 Sign Language Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tLbEEPJNxnY"
      },
      "source": [
        "## 1.1 Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rx77g9raN0tw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da07f673-21cd-44aa-d8f4-7d920e3422c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow>=2.10.0 in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Collecting tensorflow>=2.10.0\n",
            "  Downloading tensorflow-2.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (524.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m524.1/524.1 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.1.21 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10.0) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10.0) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10.0) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10.0) (1.56.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10.0) (3.8.0)\n",
            "Collecting keras<2.14,>=2.13.1 (from tensorflow>=2.10.0)\n",
            "  Downloading keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10.0) (16.0.0)\n",
            "Requirement already satisfied: numpy<=1.24.3,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10.0) (1.22.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10.0) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10.0) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10.0) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10.0) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10.0) (1.16.0)\n",
            "Collecting tensorboard<2.14,>=2.13 (from tensorflow>=2.10.0)\n",
            "  Downloading tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-estimator<2.14,>=2.13.0 (from tensorflow>=2.10.0)\n",
            "  Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.8/440.8 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10.0) (2.3.0)\n",
            "Collecting typing-extensions<4.6.0,>=3.6.6 (from tensorflow>=2.10.0)\n",
            "  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10.0) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10.0) (0.32.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow>=2.10.0) (0.40.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow>=2.10.0) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow>=2.10.0) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow>=2.10.0) (3.4.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow>=2.10.0) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow>=2.10.0) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow>=2.10.0) (2.3.6)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow>=2.10.0) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow>=2.10.0) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow>=2.10.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow>=2.10.0) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow>=2.10.0) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow>=2.10.0) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow>=2.10.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow>=2.10.0) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow>=2.10.0) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow>=2.10.0) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow>=2.10.0) (3.2.2)\n",
            "Installing collected packages: typing-extensions, tensorflow-estimator, keras, tensorboard, tensorflow\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.6.3\n",
            "    Uninstalling typing_extensions-4.6.3:\n",
            "      Successfully uninstalled typing_extensions-4.6.3\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.12.0\n",
            "    Uninstalling tensorflow-estimator-2.12.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.12.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.12.0\n",
            "    Uninstalling keras-2.12.0:\n",
            "      Successfully uninstalled keras-2.12.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.12.3\n",
            "    Uninstalling tensorboard-2.12.3:\n",
            "      Successfully uninstalled tensorboard-2.12.3\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.12.0\n",
            "    Uninstalling tensorflow-2.12.0:\n",
            "      Successfully uninstalled tensorflow-2.12.0\n",
            "Successfully installed keras-2.13.1 tensorboard-2.13.0 tensorflow-2.13.0 tensorflow-estimator-2.13.0 typing-extensions-4.5.0\n"
          ]
        }
      ],
      "source": [
        "# The way this tutorial uses the `TimeDistributed` layer requires TF>=2.10\n",
        "!pip install -U \"tensorflow>=2.10.0\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7TLB0jkWXXL",
        "outputId": "3d047371-e5ad-42a6-9929-3e5e094cee5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-PS63XTN2LF",
        "outputId": "e522c7ef-c9c5-4f9f-daca-6c504b545ee3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting remotezip\n",
            "  Downloading remotezip-0.12.1.tar.gz (7.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.65.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.7.0.72)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from remotezip) (2.27.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from remotezip) (0.8.10)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.22.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->remotezip) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->remotezip) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->remotezip) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->remotezip) (3.4)\n",
            "Building wheels for collected packages: remotezip\n",
            "  Building wheel for remotezip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for remotezip: filename=remotezip-0.12.1-py3-none-any.whl size=7933 sha256=e483ffbc7306b2b1ed1734733c2c9534726c18b85a32d596b678819a1bc7c4ba\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/76/04/beed1a6df4eb7430ee13c3900746edd517e5e597298d1f73f3\n",
            "Successfully built remotezip\n",
            "Installing collected packages: remotezip\n",
            "Successfully installed remotezip-0.12.1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for tensorflow-docs (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install remotezip tqdm opencv-python\n",
        "!pip install -q git+https://github.com/tensorflow/docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OqSUwDYENpSP"
      },
      "outputs": [],
      "source": [
        "import tqdm\n",
        "import random\n",
        "import pathlib\n",
        "import itertools\n",
        "import collections\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import remotezip as rz\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# Some modules to display an animation using imageio.\n",
        "import imageio\n",
        "from IPython import display\n",
        "from urllib import request\n",
        "from tensorflow_docs.vis import embed"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Setup Test Dir"
      ],
      "metadata": {
        "id": "b19iHn_c0lND"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "video_path_for_generator = \"/content/drive/MyDrive/test\"\n",
        "# video_file_path = video_path_for_generator +\n",
        "video_file_path = \"/content/test_video/\"        # \"/content/africa_basketball_sleep.mp4\"\n",
        "video_path_for_generator = pathlib.Path(video_path_for_generator)"
      ],
      "metadata": {
        "id": "yg3N3qWu0_Hl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# connect to google drive --> allows to store kaggle.json API token under the main folder in the drive,\n",
        "#   so that it doesn't have to be uploaded every time\n",
        "# Alternatively the kaggle.json can be uploaded under /content/\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVBVee9A66hU",
        "outputId": "1d4f6fec-a3bd-436e-a038-ada4c824b0de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JM7N0kfROReu"
      },
      "source": [
        "## 1.4 preprocess video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Q35WnswKW-F"
      },
      "outputs": [],
      "source": [
        "def format_frames(frame, output_size):\n",
        "  \"\"\"\n",
        "    Pad and resize an image from a video.\n",
        "\n",
        "    Args:\n",
        "      frame: Image that needs to resized and padded.\n",
        "      output_size: Pixel size of the output frame image.\n",
        "\n",
        "    Return:\n",
        "      Formatted frame with padding of specified output size.\n",
        "  \"\"\"\n",
        "  frame = tf.image.convert_image_dtype(frame, tf.float32)\n",
        "  frame = tf.image.resize_with_pad(frame, *output_size)\n",
        "  return frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqq7-B4QOVPd"
      },
      "outputs": [],
      "source": [
        "def frames_from_video_file(video_path, n_frames, output_size = (224,224), frame_step = 15):\n",
        "  \"\"\"\n",
        "    Creates frames from each video file present for each category.\n",
        "\n",
        "    Args:\n",
        "      video_path: File path to the video.\n",
        "      n_frames: Number of frames to be created per video file.\n",
        "      output_size: Pixel size of the output frame image.\n",
        "\n",
        "    Return:\n",
        "      An NumPy array of frames in the shape of (n_frames, height, width, channels).\n",
        "  \"\"\"\n",
        "  # Read each video frame by frame\n",
        "  result = []\n",
        "  src = cv2.VideoCapture(str(video_path))\n",
        "\n",
        "  video_length = src.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "\n",
        "  need_length = 1 + (n_frames - 1) * frame_step\n",
        "\n",
        "  if need_length > video_length:\n",
        "    start = 0\n",
        "  else:\n",
        "    max_start = video_length - need_length\n",
        "    start = random.randint(0, max_start + 1)\n",
        "\n",
        "  src.set(cv2.CAP_PROP_POS_FRAMES, start)\n",
        "  # ret is a boolean indicating whether read was successful, frame is the image itself\n",
        "  ret, frame = src.read()\n",
        "  result.append(format_frames(frame, output_size))\n",
        "\n",
        "  for _ in range(n_frames - 1):\n",
        "    for _ in range(frame_step):\n",
        "      ret, frame = src.read()\n",
        "    if ret:\n",
        "      frame = format_frames(frame, output_size)\n",
        "      result.append(frame)\n",
        "    else:\n",
        "      result.append(np.zeros_like(result[0]))\n",
        "  src.release()\n",
        "  result = np.array(result)[..., [2, 1, 0]]\n",
        "\n",
        "  return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfhVvmKHOxym"
      },
      "source": [
        "## 1.5 Create Dataset Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJxeriafO01h"
      },
      "outputs": [],
      "source": [
        "class FrameGenerator:\n",
        "  def __init__(self, path, n_frames, training = False):\n",
        "    \"\"\" Returns a set of frames with their associated label.\n",
        "\n",
        "      Args:\n",
        "        path: Video file paths.\n",
        "        n_frames: Number of frames.\n",
        "        training: Boolean to determine if training dataset is being created.\n",
        "    \"\"\"\n",
        "    self.path = path\n",
        "    self.n_frames = n_frames\n",
        "    self.training = training\n",
        "    self.class_names = sorted(set(p.name for p in self.path.iterdir() if p.is_dir()))\n",
        "    self.class_ids_for_name = dict((name, idx) for idx, name in enumerate(self.class_names))\n",
        "\n",
        "  def get_files_and_class_names(self):\n",
        "    video_paths = list(self.path.glob('*/*.mp4'))\n",
        "    classes = [p.parent.name for p in video_paths]\n",
        "    return video_paths, classes\n",
        "\n",
        "  def __call__(self):\n",
        "    video_paths, classes = self.get_files_and_class_names()\n",
        "\n",
        "    pairs = list(zip(video_paths, classes))\n",
        "\n",
        "    if self.training:\n",
        "      random.shuffle(pairs)\n",
        "\n",
        "    for path, name in pairs:\n",
        "      video_frames = frames_from_video_file(path, self.n_frames)\n",
        "      label = self.class_ids_for_name[name] # Encode labels\n",
        "      yield video_frames, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3KC9Fx5Pkug"
      },
      "outputs": [],
      "source": [
        "# Create the training set\n",
        "output_signature = (tf.TensorSpec(shape = (None, None, None, 3), dtype = tf.float32),\n",
        "                    tf.TensorSpec(shape = (), dtype = tf.int16))\n",
        "test_ds =  tf.data.Dataset.from_generator(FrameGenerator(video_path_for_generator, 10, training=False),\n",
        "                                          output_signature = output_signature)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SN8nf85jO2nG"
      },
      "outputs": [],
      "source": [
        "fg = FrameGenerator(video_path_for_generator, 10, training=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVRj65-RPxZm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50ebc3e8-a472-4bea-b3df-af355a6120f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of validation set of frames: (2, 10, 224, 224, 3)\n",
            "Shape of validation labels: (2,)\n"
          ]
        }
      ],
      "source": [
        "test_ds = test_ds.batch(2)\n",
        "\n",
        "test_frames, test_labels = next(iter(test_ds))\n",
        "\n",
        "print(f'Shape of validation set of frames: {test_frames.shape}')\n",
        "print(f'Shape of validation labels: {test_labels.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vioMpw35RrNY"
      },
      "source": [
        "# 2 Modelltraining"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyIzsL1amDtz"
      },
      "source": [
        "## 2.1 Training setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJ5kxeh1tlXk"
      },
      "outputs": [],
      "source": [
        "# pip install -q git+https://github.com/tensorflow/docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4uS-S7rk0Nm"
      },
      "outputs": [],
      "source": [
        "class_num = len(fg.class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3fGQ4Xapt7iM"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow_docs.vis import embed"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_3d_data(data):\n",
        "    # Randomly crop the video.\n",
        "    crop_size = (10, 224, 224, 3)\n",
        "    data_shape = tf.shape(data)\n",
        "    data = tf.image.random_crop(data, size=(data_shape[0], crop_size[0], crop_size[1], crop_size[2], crop_size[3]))\n",
        "\n",
        "    # Randomly flip the video horizontally.\n",
        "    data = tf.map_fn(lambda x: tf.image.random_flip_left_right(x), data)\n",
        "\n",
        "    # Add more data augmentation techniques here if needed.\n",
        "\n",
        "    return data\n",
        "\n",
        "# Create a random 3D tensor representing video data.\n",
        "video_data = tf.random.normal((10, 240, 240, 3))\n",
        "\n",
        "# # Apply the preprocessing function to the video data.\n",
        "# processed_data = preprocess_3d_data(video_data)"
      ],
      "metadata": {
        "id": "wbUobbePZS76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Layer\n",
        "\n",
        "class DataAugmentationLayer(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(DataAugmentationLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return preprocess_3d_data(inputs)"
      ],
      "metadata": {
        "id": "c82Lf7u0ZQvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Tuple\n",
        "def create_preprocessing_layer(input_shape: Tuple = (10, 224, 224, 3), rescaling = True):\n",
        "    video_input = tf.keras.Input(shape=input_shape)\n",
        "    x = ZeroPadding3D(padding=((0, 0),(4,4),(4,4)))(video_input)\n",
        "    x = DataAugmentationLayer()(x)\n",
        "    if rescaling:\n",
        "      x = tf.keras.layers.Rescaling(scale=255)(x)\n",
        "    return video_input, x"
      ],
      "metadata": {
        "id": "gCn-S6ClY94n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dropout, \\\n",
        "ZeroPadding3D, ZeroPadding2D, RandomRotation, RandomCrop,RandomFlip, RandomZoom, BatchNormalization"
      ],
      "metadata": {
        "id": "kcF1-4YzZFDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_efficient_net_model(base_model_trainable: bool = True, rescaling: bool = True):\n",
        "\n",
        "  net = tf.keras.applications.EfficientNetB0(include_top = False)\n",
        "  net.trainable = base_model_trainable\n",
        "  # Example usage:\n",
        "  input_shape = (10, 224, 224, 3)\n",
        "  video_input, x = create_preprocessing_layer(input_shape, rescaling = rescaling)\n",
        "\n",
        "  x = tf.keras.layers.TimeDistributed(net)(x)\n",
        "  x = tf.keras.layers.Dense(100)(x)\n",
        "  x = tf.keras.layers.GlobalAveragePooling3D()(x)\n",
        "  return tf.keras.Model(inputs=video_input, outputs=x)\n",
        "\n",
        "model = create_efficient_net_model(base_model_trainable = True, rescaling = True)"
      ],
      "metadata": {
        "id": "1WWt7WUwzHIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 Detection and Recognition with Pre-Trained model\n"
      ],
      "metadata": {
        "id": "m9egC3vpadpG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Top 100 classes"
      ],
      "metadata": {
        "id": "4yjezPX8A8qG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CLASS_NAMES = ['accident', 'africa', 'all', 'apple', 'basketball', 'bed', 'before', 'bird', 'birthday', 'black', 'blue', 'book', 'bowling', 'brown', 'but', 'can', 'candy', 'chair', 'change', 'cheat', 'city', 'clothes', 'color', 'computer', 'cook', 'cool', 'corn', 'cousin', 'cow', 'dance', 'dark', 'deaf', 'decide', 'doctor', 'dog', 'drink', 'eat', 'enjoy', 'family', 'fine', 'finish', 'fish', 'forget', 'full', 'give', 'go', 'graduate', 'hat', 'hearing', 'help', 'hot', 'how', 'jacket', 'kiss', 'language', 'last', 'later', 'letter', 'like', 'man', 'many', 'medicine', 'meet', 'mother', 'need', 'no', 'now', 'orange', 'paint', 'paper', 'pink', 'pizza', 'play', 'pull', 'purple', 'right', 'same', 'school', 'secretary', 'shirt', 'short', 'son', 'study', 'table', 'tall', 'tell', 'thanksgiving', 'thin', 'thursday', 'time', 'walk', 'want', 'what', 'white', 'who', 'woman', 'work', 'wrong', 'year', 'yes']"
      ],
      "metadata": {
        "id": "I-L2mYedA_C3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Detection Functions\n"
      ],
      "metadata": {
        "id": "3BQP3lRzzX0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install moviepy\n",
        "! pip install mediapipe"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFZZwznNdJt-",
        "outputId": "b45be07f-cbb4-474f-a78a-440c1a19d77c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.10/dist-packages (1.0.3)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.4.2)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.65.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.27.1)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.1.10)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from moviepy) (1.22.4)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.25.1)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.4.8)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio<3.0,>=2.5->moviepy) (8.4.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.4)\n",
            "Collecting mediapipe\n",
            "  Downloading mediapipe-0.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.8/33.8 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (23.1.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (23.5.26)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.22.4)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.10/dist-packages (from mediapipe) (4.7.0.72)\n",
            "Requirement already satisfied: protobuf<4,>=3.11 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (3.20.3)\n",
            "Collecting sounddevice>=0.4.4 (from mediapipe)\n",
            "  Downloading sounddevice-0.4.6-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.10/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.15.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (4.40.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (3.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (2.8.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.21)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n",
            "Installing collected packages: sounddevice, mediapipe\n",
            "Successfully installed mediapipe-0.10.1 sounddevice-0.4.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import detection librarys\n",
        "import cv2\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from moviepy.editor import VideoFileClip\n",
        "from moviepy.video.fx.all import crop\n",
        "\n",
        "mp_face_mesh = mp.solutions.face_mesh"
      ],
      "metadata": {
        "id": "W2U9lEsBagZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mediapipe_detection(image, model):\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    image.flags.writeable = False\n",
        "    results = model.process(image)\n",
        "    image.flags.writeable = True\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "    return image, results\n",
        "\n",
        "\n",
        "def draw_landmarks(image, results):\n",
        "    mp_holistic = mp.solutions.holistic  # Holistic model\n",
        "    mp_drawing = mp.solutions.drawing_utils  # Drawing utilities\n",
        "\n",
        "    # Draw left hand connections\n",
        "    image_new = mp_drawing.draw_landmarks(\n",
        "        image,\n",
        "        landmark_list=results.left_hand_landmarks,\n",
        "        connections=mp_holistic.HAND_CONNECTIONS,\n",
        "        landmark_drawing_spec=mp_drawing.DrawingSpec(\n",
        "            color=(232, 254, 255), thickness=1, circle_radius=4\n",
        "        ),\n",
        "        connection_drawing_spec=mp_drawing.DrawingSpec(\n",
        "            color=(255, 249, 161), thickness=2, circle_radius=2\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    image = image_new if image_new is not None else image\n",
        "    # Draw right hand connections\n",
        "    image_new = mp_drawing.draw_landmarks(\n",
        "        image,\n",
        "        landmark_list=results.right_hand_landmarks,\n",
        "        connections=mp_holistic.HAND_CONNECTIONS,\n",
        "        landmark_drawing_spec=mp_drawing.DrawingSpec(\n",
        "            color=(232, 254, 255), thickness=1, circle_radius=4\n",
        "        ),\n",
        "        connection_drawing_spec=mp_drawing.DrawingSpec(\n",
        "            color=(255, 249, 161), thickness=2, circle_radius=2\n",
        "        ),\n",
        "    )\n",
        "    image = image_new if image_new is not None else image\n",
        "\n",
        "    return image\n",
        "\n",
        "\n",
        "def cut_videos(start_frame, end_frame, video_path, counter, min_x, max_x, min_y, max_y ):\n",
        "    cap_temp = cv2.VideoCapture(video_path)\n",
        "\n",
        "\n",
        "    output_path = video_path.split(\".\")[0] + str(counter) + \".\" + video_path.split(\".\")[1]\n",
        "\n",
        "    # Get the frames per second (fps) and frame count of the video\n",
        "    fps = cap_temp.get(cv2.CAP_PROP_FPS)\n",
        "    frame_count = cap_temp.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "\n",
        "    # Set the start and end frame numbers\n",
        "    start_frame_num = start_frame\n",
        "    end_frame_num = end_frame\n",
        "\n",
        "    # Check if the specified frames are within the video's range\n",
        "    if start_frame_num > frame_count or end_frame_num > frame_count:\n",
        "        print(\"Invalid frame range.\")\n",
        "        cap_temp.release()\n",
        "        exit()\n",
        "\n",
        "    # Set the start frame position\n",
        "    cap_temp.set(cv2.CAP_PROP_POS_FRAMES, start_frame_num)\n",
        "\n",
        "    # Create a VideoWriter object to write the extracted frames\n",
        "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "    output = cv2.VideoWriter(output_path, fourcc, fps, (int(cap_temp.get(3)), int(cap_temp.get(4))))\n",
        "\n",
        "    # Read and write the frames within the specified range\n",
        "    current_frame = start_frame_num\n",
        "\n",
        "    while current_frame <= end_frame_num:\n",
        "        ret, frame = cap_temp.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        height, width = frame.shape[:2]\n",
        "        # print(height, width)\n",
        "        # cropped_frame = frame[int(min_x*width):int(max_x*width), int(min_y*height):int(max_y*height)]\n",
        "\n",
        "        # print(cropped_frame.shape)\n",
        "\n",
        "        output.write(frame)\n",
        "        current_frame += 1\n",
        "\n",
        "    cap_temp.release()\n",
        "\n",
        "    return {output_path: [int(min_x*width), int(min_y*height), int(max_x*width), int(max_y*height)]}\n",
        "    # cap.release()\n",
        "    # clip = VideoFileClip(output_path)\n",
        "    # new_clip = crop(clip, x1=int(min_x*width), y1=int(min_y*height), x2=int(max_x*width), y2=int(max_y*height))\n",
        "    # target_path_cropped = output_path.split(\".\")[0] + \"cropped.\" + output_path.split(\".\")[1]\n",
        "    # print(new_clip.write_videofile(target_path_cropped, codec='mpeg4', audio=False))\n",
        "\n",
        "\n",
        "def filter_min_max(lst):\n",
        "    x_vals = [i.x for i in lst]\n",
        "    y_vals = [i.y for i in lst]\n",
        "    min_x = min(x_vals)\n",
        "    max_x = max(x_vals)\n",
        "    min_y = min(y_vals)\n",
        "    max_y = max(y_vals)\n",
        "    return [min_x, max_x, min_y, max_y]\n",
        "\n",
        "\n",
        "def get_min_max_of_face(frame):\n",
        "    with mp_face_mesh.FaceMesh(static_image_mode=True,\n",
        "                           max_num_faces=1,\n",
        "                        #    refine_landmarks=True,\n",
        "                           min_detection_confidence=0.5) as face_mesh:\n",
        "\n",
        "        results = face_mesh.process(frame)\n",
        "\n",
        "    if bool(results.multi_face_landmarks):\n",
        "        face_landmarks = results.multi_face_landmarks[0]\n",
        "        face_coordinates = [face_landmark for face_landmark in face_landmarks.landmark]\n",
        "        return filter_min_max(face_coordinates)\n",
        "    else:\n",
        "        return [None, None, None, None]\n",
        "\n",
        "\n",
        "def create_empty_dataframe():\n",
        "    return pd.DataFrame(columns=[\"left_hand_x_min\", \"left_hand_x_max\", \"left_hand_y_min\", \"left_hand_y_max\",\n",
        "                                                    \"right_hand_x_min\", \"right_hand_x_max\", \"right_hand_y_min\", \"right_hand_y_max\",\n",
        "                                                    \"face_x_min\", \"face_x_max\", \"face_y_min\", \"face_y_max\"])\n",
        "\n",
        "\n",
        "def crop_videos_from_dataframe(df: pd.DataFrame, cap):\n",
        "    # for item in [\"left_hand_x_min\", \"left_hand_x_max\", \"right_hand_x_min\", \"right_hand_x_max\"]:\n",
        "    #     df[item].fillna(df[\"face_x_min\"].dropna().max())\n",
        "\n",
        "    # for item in [\"left_hand_y_max\", \"left_hand_y_min\", \"right_hand_y_max\", \"right_hand_y_min\"]:\n",
        "    #     df[item].fillna(df[\"face_y_min\"].dropna().max())\n",
        "\n",
        "    # df[\"left_hand_x_max\"].fillna(df[\"face_x_max\"].dropna().max())\n",
        "    min_x = np.nanmin(np.array((df[\"left_hand_x_min\"].dropna().min(),df[\"face_x_min\"].dropna().min(), df[\"right_hand_x_min\"].dropna().min())))\n",
        "    min_y = np.nanmin(np.array((df[\"left_hand_y_min\"].dropna().min(),df[\"face_y_min\"].dropna().min(), df[\"right_hand_y_min\"].dropna().min())))\n",
        "    max_x = np.nanmax(np.array((df[\"left_hand_x_max\"].dropna().max(),df[\"face_x_max\"].dropna().max(), df[\"right_hand_x_max\"].dropna().max())))\n",
        "    max_y = np.nanmax(np.array((df[\"left_hand_y_max\"].dropna().max(),df[\"face_y_max\"].dropna().max(), df[\"right_hand_y_max\"].dropna().max())))\n",
        "    # min_y = min(min(df[\"left_hand_y_min\"]), min(df[\"face_y_min\"]), min(df[\"right_hand_y_min\"]))\n",
        "    # max_x = max(max(df[\"left_hand_x_max\"]), max(df[\"face_x_max\"]), max(df[\"right_hand_x_max\"]))\n",
        "    # max_y = max(max(df[\"left_hand_y_max\"]), max(df[\"face_y_max\"]), max(df[\"right_hand_y_max\"]))\n",
        "    face_middle = (df[\"face_x_min\"].dropna().mean() + df[\"face_x_max\"].dropna().mean())/2\n",
        "    distance = max(face_middle-min_x, max_x-face_middle)\n",
        "    # crop_video(face_middle-distance, min_y, face_middle+distance, max_y)\n",
        "    min_x = face_middle-distance-0.1 if face_middle-distance-0.1 > 0 else 0\n",
        "    max_x = face_middle+distance+0.1  if face_middle+distance+0.1 < 1 else 1\n",
        "    min_y = min_y-0.1 if min_y-0.1 > 0 else 0\n",
        "    max_y = max_y+0.1 if max_y+0.1 < 1 else 1\n",
        "    print(face_middle-distance, face_middle+distance, min_y, max_y)\n",
        "    return face_middle-distance, face_middle+distance, min_y, max_y"
      ],
      "metadata": {
        "id": "82oycJp8au2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# video_path = pathlib.Path(video_path)\n",
        "\n",
        "cap = cv2.VideoCapture(video_file_path)\n",
        "\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "threshold_counter_no_hand = fps // 2\n",
        "\n",
        "video_times = []\n",
        "crop_video = []\n",
        "video_postions = create_empty_dataframe()\n",
        "start_time_temp = None\n",
        "counter_no_hand = None\n",
        "# end_time_temp = None\n",
        "frame_counter = 0\n",
        "\n",
        "# Check if camera opened successfully\n",
        "if (cap.isOpened()== False):\n",
        "  print(\"Error opening video stream or file\")\n",
        "\n",
        "with mp.solutions.holistic.Holistic(\n",
        "        min_detection_confidence=0.5, min_tracking_confidence=0.5\n",
        "    ) as holistic:\n",
        "  # Read until video is completed\n",
        "  while(cap.isOpened()):\n",
        "    # Capture frame-by-frame\n",
        "    ret, frame = cap.read()\n",
        "    if ret == True:\n",
        "      frame_counter += 1\n",
        "\n",
        "      # Make detections\n",
        "      image, results = mediapipe_detection(frame, holistic)\n",
        "      if results is not None:\n",
        "        if results.left_hand_landmarks is not None:\n",
        "          video_postions_temp = filter_min_max([hand_landmarks for hand_landmarks in results.left_hand_landmarks.landmark])\n",
        "        else:\n",
        "          video_postions_temp = [None, None, None, None]\n",
        "        if results.right_hand_landmarks is not None:\n",
        "          video_postions_temp.extend(filter_min_max([hand_landmarks for hand_landmarks in results.right_hand_landmarks.landmark]))\n",
        "        else:\n",
        "          video_postions_temp.extend([None, None, None, None])\n",
        "\n",
        "        video_postions_temp.extend(get_min_max_of_face(frame))\n",
        "        video_postions_temp_df = pd.DataFrame([video_postions_temp],\n",
        "                                           columns=[\"left_hand_x_min\", \"left_hand_x_max\", \"left_hand_y_min\", \"left_hand_y_max\",\n",
        "                                                    \"right_hand_x_min\", \"right_hand_x_max\", \"right_hand_y_min\", \"right_hand_y_max\",\n",
        "                                                    \"face_x_min\", \"face_x_max\", \"face_y_min\", \"face_y_max\"])\n",
        "        video_postions = pd.concat([video_postions, video_postions_temp_df], ignore_index=True)\n",
        "\n",
        "      if results.right_hand_landmarks is None and results.left_hand_landmarks is None:\n",
        "\n",
        "        if counter_no_hand is not None:\n",
        "          counter_no_hand += 1\n",
        "\n",
        "          if counter_no_hand > threshold_counter_no_hand:\n",
        "            # if end_time_temp is None:\n",
        "            if (frame_counter - start_time_temp) > fps:\n",
        "              video_times.append((start_time_temp, frame_counter))\n",
        "              min_x, max_x, min_y, max_y = crop_videos_from_dataframe(video_postions, cap)\n",
        "              video_postions = create_empty_dataframe()\n",
        "              crop_video.append(cut_videos(start_time_temp, frame_counter, video_file_path, len(video_times), min_x, max_x, min_y, max_y))\n",
        "              start_time_temp = None\n",
        "              counter_no_hand = None\n",
        "\n",
        "      else:\n",
        "        if start_time_temp is None:\n",
        "          start_time_temp = frame_counter\n",
        "          counter_no_hand = 0\n",
        "\n",
        "      if results is not None and image is not None:\n",
        "        image = draw_landmarks(image, results)\n",
        "\n",
        "      # # Display the resulting frame\n",
        "      # cv2.imshow('Image', image)\n",
        "\n",
        "\n",
        "      # # Press Q on keyboard to  exit\n",
        "      # if cv2.waitKey(25) & 0xFF == ord('q'):\n",
        "      #   break\n",
        "\n",
        "    # Break the loop\n",
        "    else:\n",
        "      break\n",
        "\n",
        "if start_time_temp is not None:\n",
        "  if (frame_counter - start_time_temp) > fps:\n",
        "    video_times.append((start_time_temp, frame_counter))\n",
        "    min_x, max_x, min_y, max_y = crop_videos_from_dataframe(video_postions, cap)\n",
        "    video_postions = create_empty_dataframe()\n",
        "    crop_video.append(cut_videos(start_time_temp, frame_counter, video_file_path, len(video_times), min_x, max_x, min_y, max_y))\n",
        "\n",
        "# When everything done, release the video capture object\n",
        "cap.release()\n",
        "\n",
        "for item in crop_video:\n",
        "    for key, value in item.items():\n",
        "        clip = VideoFileClip(key)\n",
        "        new_clip = crop(clip, x1=value[0], y1=value[1], x2=value[2], y2=value[3])\n",
        "        target_path_cropped = key.split(\".\")[0] + \"cropped.\" + key.split(\".\")[1]\n",
        "        print(new_clip.write_videofile(target_path_cropped, audio=False))   # , codec='mpeg4'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xykXW13haz6m",
        "outputId": "bad48619-f5b0-42d6-d64a-4ceca21e6b84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error opening video stream or file\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3. Load Recognition Model"
      ],
      "metadata": {
        "id": "7QyMZs5fzpJB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Model has to be stored under the root dir in drive\n",
        "- this can be achieved by copying the model and set the dir to \"Meine Ablagen\""
      ],
      "metadata": {
        "id": "emgbq7LyzwCD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_efficient_net_model()\n",
        "\n",
        "checkpoint_path = pathlib.Path(\"/content/drive/MyDrive/checkpoints_EfficientNetB0_15epochs\")\n",
        "\n",
        "model.compile(optimizer = 'adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(test_ds.take(1),\n",
        "          epochs = 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNgsj32AfBAZ",
        "outputId": "0ed27cf9-37e5-4c76-b019-a09e5b57111b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 57s 57s/step - loss: 4.4126 - accuracy: 0.0000e+00\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7f46d4acd600>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loads the weights\n",
        "model.load_weights(\"/content/drive/MyDrive/checkpoints_EfficientNetB0_2_epochs/my_checkpoint\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObsgyvJ1iyqp",
        "outputId": "4e8284f9-e23f-4340-ad82-7618609a443a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7f46e8d21930>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def to_gif(images):\n",
        "  converted_images = np.clip(images * 255, 0, 255).astype(np.uint8)\n",
        "  imageio.mimsave('./animation.gif', converted_images, fps=10)\n",
        "  return embed.embed_file('./animation.gif')"
      ],
      "metadata": {
        "id": "fy0xBRTh-tK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display\n",
        "from IPython.display import Video\n",
        "\n",
        "# Create an instance of FrameGenerator for the chosen dataset\n",
        "chosen_fg = FrameGenerator(pathlib.Path(video_file_path), 10)\n",
        "\n",
        "# Select random video and true label\n",
        "chosen_frames, true_label = random.choice(list(chosen_fg()))\n",
        "\n",
        "# model prediction\n",
        "chosen_frames_expanded = np.expand_dims(chosen_frames, axis=0)\n",
        "predicted_label = np.argmax(model.predict(chosen_frames_expanded), axis=-1)\n",
        "\n",
        "# Get the true label's class name\n",
        "true_class_name = chosen_fg.class_names[true_label]\n",
        "\n",
        "#get the predicted labels class name\n",
        "# CLASS_NAMES = fg.class_names\n",
        "predicted_class_name = \"test\" # CLASS_NAMES[predicted_label[0]]\n",
        "\n",
        "# print(f\"True label: {true_label} ({true_class_name})\")\n",
        "print(f\"Predicted label: {predicted_label[0]} ({predicted_class_name})\")\n",
        "\n",
        "# # Display the video\n",
        "# random_video_path = None\n",
        "for path, name in zip(*chosen_fg.get_files_and_class_names()):\n",
        "    if chosen_fg.class_ids_for_name[name] == true_label:\n",
        "        random_video_path = str(path)\n",
        "        break\n",
        "\n",
        "# if random_video_path:\n",
        "#     display(Video(to_gif(sample_video), embed=True))\n",
        "# else:\n",
        "#     print(\"Error: video not found.\")\n",
        "sample_video = frames_from_video_file(pathlib.Path(random_video_path), n_frames = 10)\n",
        "to_gif(sample_video)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "ARtkCQ9DbOVA",
        "outputId": "91419e0a-4982-4ddc-f285-6108a7dbfb47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-b4b68243de1c>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Create an instance of FrameGenerator for the chosen dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mchosen_fg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFrameGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Select random video and true label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-0daf1d354b11>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, n_frames, training)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_frames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_frames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_ids_for_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-0daf1d354b11>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_frames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_frames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_ids_for_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/pathlib.py\u001b[0m in \u001b[0;36miterdir\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1015\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mspecial\u001b[0m \u001b[0mpaths\u001b[0m \u001b[0;34m'.'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'..'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m         \"\"\"\n\u001b[0;32m-> 1017\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1018\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'..'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m                 \u001b[0;31m# Yielding a path object for these makes little sense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/test_video'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# display(Video(\"/content/africa_basketball_sleep.mp4\", embed=True))"
      ],
      "metadata": {
        "id": "Ih4sRE4Emaw2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}